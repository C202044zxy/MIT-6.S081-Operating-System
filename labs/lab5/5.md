## Copy-on-Write fork

COW fork creates just a pagetable for the child, with PTEs for user memory pointing to the parent's physical pages. 

```c
// vm.c
int
uvmcopy(pagetable_t old, pagetable_t new, uint64 sz)
{
  pte_t *pte;
  uint64 pa, i;
  uint flags;

  for(i = 0; i < sz; i += PGSIZE){
    if((pte = walk(old, i, 0)) == 0)
      panic("uvmcopy: pte should exist");
    if((*pte & PTE_V) == 0)
      panic("uvmcopy: page not present");
    *pte |= PTE_COW;  // enable Copy-on-write
    *pte &= ~PTE_W;   // disable write
    pa = PTE2PA(*pte);
    flags = PTE_FLAGS(*pte);
    // Map with the same PA
    if (mappages(new, i, PGSIZE, (uint64)pa, flags) != 0) {
      goto err;
    }
    refinc(pa); // increase the reference count of PA
  }
  return 0;

 err:
  uvmunmap(new, 0, i / PGSIZE, 1);
  return -1;
}
```

When either process tries to write one of these COW pages, the CPU will force a page fault. The kernel page-fault handler detects this case, allocates a page of physical memory for the faulting process, copies the original page into the new page, and modifies the relevant PTE in the faulting process to refer to the new page, this time with the PTE marked writeable.

```c
// trap.c

// in usertrap
...
else if (r_scause() == 15) {
	// detect page fault
	// get the virtual memory
	uint64 va = r_stval();
  if (cowhandler(va) != 0) {
		p->killed = 1;
	}
}
...

// handle the page-fault CAUSED BY COW
int cowhandler(uint64 va) {
  if (va >= MAXVA) {
    return -1;
  }
  acquire(&cowlock);
  struct proc *p = myproc();
  // make va page-aligned
  va = PGROUNDDOWN(va);
  pte_t *pte;
  if ((pte = walk(p->pagetable, va, 0)) == 0) {
    // no page table entry
    acquire(&cowlock);
    return -1;
  }
  // check for whether cow bit exist, whether pte is valid
  if (!(*pte & PTE_COW) || !(*pte & PTE_V)) {
    release(&cowlock);
    return -1;
  }
  uint64 pa = PTE2PA(*pte);
  uint64 flags = (PTE_FLAGS(*pte) & (~PTE_COW)) | PTE_W;
  // get the reference count
  uint64 refcount = fetchref(pa);
  if (refcount == 1) {
    *pte &= ~PTE_COW; // disable cow
    *pte |= PTE_W;    // enable write
    release(&cowlock);
    return 0;
  }
  char* mem;
  if ((mem = kalloc()) == 0) {
    release(&cowlock);
    return -1;
  }
  // copy data in physical address
  memmove(mem, (char*)pa, PGSIZE);
  // free the old page
  // decrease the reference count automatically
  uvmunmap(p->pagetable, va, 1, 1); 
  // map the new page
  if (mappages(p->pagetable, va, PGSIZE, (uint64)mem, flags) != 0){
    kfree(mem);
    release(&cowlock);
    return -1;
  }
  release(&cowlock);
  return 0;
}
```

For each physical page, keep a reference count of the number of user page tables that refer to that page.

```c
// kalloc.c
void refinc(void *pa) {
  if(((uint64)pa % PGSIZE) != 0 || (char*)pa < end || (uint64)pa >= PHYSTOP)
    panic("refinc");
  acquire(&kmem.reflock);
  kmem.ref[PA2IND((uint64)pa)]++;
  release(&kmem.reflock);
}

void refdec(void *pa) {
  if(((uint64)pa % PGSIZE) != 0 || (char*)pa < end || (uint64)pa >= PHYSTOP)
    panic("refdec");
  acquire(&kmem.reflock);
  kmem.ref[PA2IND((uint64)pa)]--;
  release(&kmem.reflock);
}

uint64 fetchref(void *pa) {
  if(((uint64)pa % PGSIZE) != 0 || (char*)pa < end || (uint64)pa >= PHYSTOP)
    panic("fetchref");
  acquire(&kmem.reflock);
  uint64 res = kmem.ref[PA2IND((uint64)pa)];
  release(&kmem.reflock);
  return res;
}

void
kfree(void *pa)
{
  struct run *r;

  if(((uint64)pa % PGSIZE) != 0 || (char*)pa < end || (uint64)pa >= PHYSTOP)
    panic("kfree");

  uint64 ind = PA2IND((uint64)pa);
  acquire(&kmem.reflock);
  if (kmem.ref[ind] > 0) {
    kmem.ref[ind]--;
    if (kmem.ref[ind] > 0) {
      release(&kmem.reflock);
      return;
    }
  }
  // delete only when reference count = 0
  release(&kmem.reflock);
  ...
}

void *
kalloc(void)
{
  struct run *r;

  acquire(&kmem.lock);
  r = kmem.freelist;
  if(r) {
    kmem.freelist = r->next;
    refinc(r);
  }
  release(&kmem.lock);
  ...
}
```

